{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Techforum : Deep Learning (part 2/3)\n",
    "\n",
    "## Simple Softmax Regression using Keras\n",
    "\n",
    "Objective:\n",
    "- Discover [Keras](https://keras.io/) : a high-level neural networks API running on top of [TensorFlow](https://www.tensorflow.org/) \n",
    "- See how it make it easier/faster to write code than using directly Tensorflow :\n",
    "    - Same example as in part 1, but rewrited to use Keras APIs\n",
    "\n",
    "Note : this toy-example is still ok to run on a CPU laptop :)\n",
    "    \n",
    "Next : Improve the accuracy results using Deep Learning : Convolutional Neural Networks (Convnets) in Keras\n",
    "\n",
    "Notebook inspired by : https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import _dotparser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import timeit\n",
    "\n",
    "# Use Tensorflow tutorial's helper to load/prepare the MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout   # Keras Building blocks for Deep Neural Net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import data (Thanks to helpers provided in Tensorflow tutorials !)\n",
    "mnist = input_data.read_data_sets('./', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define for convenience a few (Python/Numpy)variables to handle the dataset \n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not important : Just a counter to separate logs directory between each training experiments\n",
    "experiments = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some Hyperparameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How fast the network will learn, by making more or less small updates during training\n",
    "#    too low, and the network will take too much time to learn\n",
    "#    too high, and the network might never converge to a solution\n",
    "learning_rate = 0.5 \n",
    "\n",
    "# Number of training epoch (in Keras : ~loop on the full training dataset)\n",
    "epoch = 5 \n",
    "\n",
    "# Number of images to process per batch iteration\n",
    "batch_size = 100\n",
    "\n",
    "# Path to home of the Tensorboard logs and Training Checkpoints\n",
    "logs_path = \"./logs/mnist/Keras/softmaxReg\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build the model : just by stacking the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Sequential model is a linear stack of layers\n",
    "model = Sequential()\n",
    "\n",
    "# Build the same Softmax Regression network as in the example part 1\n",
    "# Dense is just a regular densely-connected NN layer, here with 10 neurons\n",
    "# and we apply a softmax activation on them\n",
    "model.add(Dense(10, activation='softmax', input_shape=(784,)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cool tool to display information about the model we have built\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training in Keras : compile, fit (train), evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a callback to write a log for TensorBoard (called by model.fit())\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir=str(logs_path + str(experiments)), \n",
    "                                         histogram_freq=0, \n",
    "                                         write_graph=True, \n",
    "                                         write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Monitor execution time\n",
    "start_time = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.4012 - acc: 0.8851 - val_loss: 0.3004 - val_acc: 0.9122\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.3089 - acc: 0.9125 - val_loss: 0.2840 - val_acc: 0.9195\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.2952 - acc: 0.9162 - val_loss: 0.2831 - val_acc: 0.9200\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.2874 - acc: 0.9188 - val_loss: 0.2748 - val_acc: 0.9232\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 2s - loss: 0.2821 - acc: 0.9206 - val_loss: 0.2792 - val_acc: 0.9210\n",
      "Test loss: 0.279151135343\n",
      "Test accuracy: 0.921\n",
      "Execution time= 14.134245 sec\n"
     ]
    }
   ],
   "source": [
    "# It possible to use one of the Keras or a Tensorflow optimizers as here :\n",
    "opt= keras.optimizers.TFOptimizer(tf.train.GradientDescentOptimizer(learning_rate=learning_rate))\n",
    "\n",
    "# Configure the model for training\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,  \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model for a fixed number of epochs (iterations on a dataset)\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[tbCallBack])\n",
    "\n",
    "# Return the loss value & metrics values for the model in test mode\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#### Training is done !\n",
    "\n",
    "print(\"Execution time= %4f sec\" % (timeit.default_timer() - start_time)) \n",
    "\n",
    "# Not important : increment our counter to avoid mixing up with our logs between experiments in Jupyter\n",
    "experiments+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Deeper ...\n",
    "\n",
    "Let's try to stack more layers to the neural Network and see how it affects the performance and computation time. see how easy it is using Keras :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#### The Sequential model is a linear stack of layers\n",
    "model = Sequential()\n",
    "\n",
    "#### Build the same Softmax Regression network as in the exampel part 1\n",
    "\n",
    "# Just a regular densely-connected NN layer, here with 512 neurons\n",
    "# relu (rectified linear unit) is applying a non-linearity \n",
    "model.add(Dense(units=512, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Add regularization : Dropout consists in randomly setting a fraction rate \n",
    "# of input units to 0 at each update during training time which helps prevent overfitting\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "# Continue to stack layers similarly\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Cool tool to display information about the model we have built\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "That's **85x** more parameters to opimizise during the training (and this is still a toy network !)0\n",
    "\n",
    "#### Let's train the updated model\n",
    "\n",
    "\n",
    "The code below is exactly  the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 20s - loss: 0.3074 - acc: 0.9072 - val_loss: 0.1209 - val_acc: 0.9625\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 19s - loss: 0.1199 - acc: 0.9635 - val_loss: 0.0859 - val_acc: 0.9734\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 19s - loss: 0.0859 - acc: 0.9726 - val_loss: 0.0889 - val_acc: 0.9735\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 19s - loss: 0.0694 - acc: 0.9777 - val_loss: 0.0773 - val_acc: 0.9766\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 19s - loss: 0.0555 - acc: 0.9820 - val_loss: 0.0630 - val_acc: 0.9805\n",
      "Test loss: 0.0629786895297\n",
      "Test accuracy: 0.9805\n",
      "Execution time= 100.837122 sec\n"
     ]
    }
   ],
   "source": [
    "# This callback writes a log for TensorBoard and is called by model.fit()\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir=str(logs_path + str(experiments)), \n",
    "                                         histogram_freq=0, \n",
    "                                         write_graph=True, \n",
    "                                         write_images=False)\n",
    "\n",
    "# It possible to use one of the Keras or a Tensorflow optimizers as shown here\n",
    "opt= keras.optimizers.TFOptimizer(tf.train.GradientDescentOptimizer(learning_rate=learning_rate))\n",
    "\n",
    "# Not important : Monitor execution time\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "#### Configure the model for training\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,  \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#### Train the model for a fixed number of epochs (iterations on a dataset)\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[tbCallBack])\n",
    "\n",
    "##### Return the loss value & metrics values for the model in test mode\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#### Training is done !\n",
    "\n",
    "print(\"Execution time= %4f sec\" % (timeit.default_timer() - start_time)) \n",
    "\n",
    "# Not important : increment our counter to avoid mixing up with our logs between experiments in Jupyter\n",
    "experiments+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
