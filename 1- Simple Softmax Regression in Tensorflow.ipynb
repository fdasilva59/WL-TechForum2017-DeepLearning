{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techforum : Deep Learning (part 1/3)\n",
    "\n",
    "## Simple Softmax regression with Tensorflow\n",
    "\n",
    "Objective:\n",
    "- Discover Tensorflow\n",
    "    - Classify handwritten digits 0-9 (MNIST dataset)\n",
    "    - Use a simple neural Network (Softmax regression)\n",
    "- Introduce Tensorboard\n",
    "\n",
    "Note : this toy-example is ok to run on a CPU laptop :)\n",
    "    \n",
    "Next : Using High Level API (Keras) to do the same job in less lines of code\n",
    "\n",
    "Notebook inspired by :https://www.tensorflow.org/get_started/mnist/beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this example :\n",
    "\n",
    "#### MNIST\n",
    "The MNIST database of handwritten digits, available from  http://yann.lecun.com/exdb/mnist/, has a training set of 60,000 examples, and a test set of 10,000 examples\n",
    " <img  src=\"assets/mnist_digits.png\" width=\"400px\"> \n",
    " \n",
    "#### Dataset of images \n",
    "Each images has a size of 28x28 pixels and has already been pre-processed (image centered, gery scale, normalization...)\n",
    "\n",
    " <img src=\"assets/MNIST-Matrix.png\" width=\"600px\">\n",
    " \n",
    "In the MNIST dataset, the images have been reshaped in to a vector of size [1, 784] so that all the images fits into a matrix [55000, 784] (training set)\n",
    "\n",
    "<img src=\"assets/mnist-train-xs.png\" width=\"300px\">\n",
    " \n",
    "The target labels (supervised learning) have been \"one-hot\" encoded, thus it's a vector of [1, 10] for one image, [55000, 10] (training set)\n",
    "\n",
    "<img src=\"assets/mnist-train-ys.png\" width=\"300px\">\n",
    "\n",
    "#### Model \n",
    "In this notebook we will implement a simple Softmax Regression model to classified the handwritten images in 10 classes, corresponfing to the 0-9 digits. \n",
    "\n",
    "The softmax functionis a generalization of the logistic function that \"squashes\" a K-dimensional vector of arbitrary real values to a K-dimensional vecto of real values in the range (0, 1] that add up to 1.\n",
    "\n",
    " <img  src=\"assets/softmax-regression-scalargraph.png\" width=\"400px\">\n",
    " \n",
    " <img  src=\"assets/network_diagram.png\" width=\"400px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "# Use Tensorflow tutorial's helper to load/prepare the MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import data (Thanks to helpers provided in Tensorflow tutorials !)\n",
    "# Data are already pre-processed and ready to use\n",
    "# All the more the training output is alredy \"one-hot\" encoded\n",
    "mnist = input_data.read_data_sets('./', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is that mnist.train.images is a tensor (an n-dimensional array) with a shape of [55000, 784]. The first dimension is an index into the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not important : Just a counter to separate logs directory between each training experiments\n",
    "experiments = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to perform another experiment, restart from here in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset the Tensorflow graph (allows multiple experiments in Jupyter)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some Hyperparameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How fast the network will learn, by making more or less small updates during training\n",
    "#    too low, and the network will take too much time to learn\n",
    "#    too high, and the network might never converge to a solution\n",
    "learning_rate = 0.5 \n",
    "\n",
    "# Number of training epoch (global iterations for the training)\n",
    "epoch = 3000\n",
    "\n",
    "# Number of images to process per batch iteration\n",
    "batch_size = 100\n",
    "\n",
    "# Path to home of the Tensorboard logs and Training Checkpoints\n",
    "logs_path = \"./logs/mnist/softmaxReg\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables\n",
    "\n",
    "In Tensorflow there are 3 types of variables (Tensors which are n-dimensional arrays):\n",
    "- tf.placeholder : which are the entry point to feed  the data set\n",
    "- tf.variable : which value can be updated during the execution\n",
    "- tf.constant : self explanatory !\n",
    "\n",
    "***keep in mind*** : To access the value of a variable, you must evaluate the variable within a session (sess.run(xxx) or xxx.eval() ), otherwise you will just get a tensor object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specifying a name_scope is not necessary, but it allows to display the Tensorflow graph nicely in Tensorboard\n",
    "with tf.name_scope('feed-dict'):\n",
    "    \n",
    "    # Create a Tensor to feed the input data  (images)\n",
    "    #   A image is 28x28 pixels, but it is store in row thus 1 x 784 pixels  \n",
    "    #   The number of images that will be fed when the session will be executed\n",
    "    #   is not known when buiding the graph of operations : thus 'None'  for the numbers of rows\n",
    "    inputs = tf.placeholder(dtype=tf.float32, shape=[None, 784], name='inputs')\n",
    "\n",
    "    # Create a Tensor to feed the real output values that the network will be trained on\n",
    "    targets = tf.placeholder(dtype= tf.float32, shape=[None, 10], name='targets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some Tensor for the weights  and bias parameters of the Neural Network\n",
    "# Here we initialize these tensors with zero values\n",
    "\n",
    "#with tf.name_scope('weights'):\n",
    "Weights = tf.Variable(tf.zeros(shape=[784, 10]), name='weights')\n",
    "\n",
    "#with tf.name_scope('biases'):    \n",
    "biases = tf.Variable(tf.zeros(shape = [10]), name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional : Allow to inspect 3 images of the dataset in Tensorboard\n",
    "with tf.name_scope('Tensorboard-img'):    \n",
    "\n",
    "    inputs_image = tf.reshape(inputs, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', inputs_image, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Evaluate the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('predictions'):  \n",
    "    predictions = tf.matmul(inputs, Weights) + biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning we typically define **what it means for a model to be bad. We call this the cost, or the loss** \n",
    "We try to minimize that error, and the smaller the error margin, the better our model is.\n",
    "One very common, very **nice function to determine the loss of a model is called \"cross-entropy\"** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope('cross-entropy'):\n",
    "    # Use cross-entropy to determine the loss/cost (error of our model)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets,\n",
    "                                                                           logits=predictions))\n",
    "\n",
    "    # Tensorboard : summary.scalar  allows to collect summary information for this \n",
    "    # node so that its evolution during the training  can be plot as a graph in Tensorboard\n",
    "    tf.summary.scalar(\"loss\", cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the training **accuracy of the model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                                                                       \n",
    "with tf.name_scope('accuracy'): \n",
    "    \n",
    "    # Argmax returns the index with the largest value across axes of a tensor.\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(targets, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    \n",
    "    # Tensorboard : summary.scalar  allows to collect summary information for this \n",
    "    # node so that its evolution during the training  can be plot as a graph in Tensorboard\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Training Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because TensorFlow knows the entire graph of the computations, it can **automatically use the backpropagation algorithm** \n",
    "to efficiently determine how the weight variables affect the loss you ask it to minimize. Then it can apply a choisen  \n",
    "optimization algorithm to modify the variables and reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope('train'): \n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0   training accuracy= 0.57   test accuracy= 0.4075\n",
      "iteration:  100   training accuracy= 0.96   test accuracy= 0.8948\n",
      "iteration:  200   training accuracy= 0.97   test accuracy= 0.9031\n",
      "iteration:  300   training accuracy= 0.97   test accuracy= 0.9074\n",
      "iteration:  400   training accuracy= 0.88   test accuracy= 0.9037\n",
      "iteration:  500   training accuracy= 0.92   test accuracy= 0.9125\n",
      "iteration:  600   training accuracy= 0.93   test accuracy= 0.917\n",
      "iteration:  700   training accuracy= 0.91   test accuracy= 0.9149\n",
      "iteration:  800   training accuracy= 0.95   test accuracy= 0.9138\n",
      "iteration:  900   training accuracy= 0.96   test accuracy= 0.915\n",
      "iteration:  1000   training accuracy= 0.95   test accuracy= 0.9157\n",
      "iteration:  1100   training accuracy= 0.94   test accuracy= 0.9196\n",
      "iteration:  1200   training accuracy= 0.93   test accuracy= 0.9194\n",
      "iteration:  1300   training accuracy= 0.95   test accuracy= 0.9174\n",
      "iteration:  1400   training accuracy= 0.96   test accuracy= 0.9211\n",
      "iteration:  1500   training accuracy= 0.91   test accuracy= 0.9189\n",
      "iteration:  1600   training accuracy= 0.93   test accuracy= 0.921\n",
      "iteration:  1700   training accuracy= 0.93   test accuracy= 0.9211\n",
      "iteration:  1800   training accuracy= 0.94   test accuracy= 0.9188\n",
      "iteration:  1900   training accuracy= 0.94   test accuracy= 0.9174\n",
      "iteration:  2000   training accuracy= 0.9   test accuracy= 0.9187\n",
      "iteration:  2100   training accuracy= 0.94   test accuracy= 0.9182\n",
      "iteration:  2200   training accuracy= 0.96   test accuracy= 0.9226\n",
      "iteration:  2300   training accuracy= 0.95   test accuracy= 0.9209\n",
      "iteration:  2400   training accuracy= 0.94   test accuracy= 0.9165\n",
      "iteration:  2500   training accuracy= 0.9   test accuracy= 0.9209\n",
      "iteration:  2600   training accuracy= 0.92   test accuracy= 0.9213\n",
      "iteration:  2700   training accuracy= 0.93   test accuracy= 0.9185\n",
      "iteration:  2800   training accuracy= 0.91   test accuracy= 0.9222\n",
      "iteration:  2900   training accuracy= 0.97   test accuracy= 0.9207\n",
      "Execution time= 7.446860 sec\n"
     ]
    }
   ],
   "source": [
    "# Open a Session in order to execute the computaions\n",
    "# on the graph of operations defined above\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    #### Prepare to write Summaries for Tensorboard (if you want to use Tensorboard)\n",
    "    \n",
    "    # create 2 log writers to logs the summaries for Tensorflow at training and at test steps\n",
    "    # that's a tricky way (not well documented) to get both training and test graphs in the \n",
    "    # same chart in Tensorboard\n",
    "    train_writer = tf.summary.FileWriter(str(logs_path + \"-train-\" +str(experiments)))\n",
    "    test_writer = tf.summary.FileWriter(str(logs_path + \"-test-\" +str(experiments)))  \n",
    "    \n",
    "    # Merge all summaries together so it's easier to manl_acc: 0.9896age\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # Allow to display the graph of computations in Tensorboard\n",
    "    train_writer.add_graph(sess.graph)\n",
    "    test_writer.add_graph(sess.graph)\n",
    "    \n",
    "    # Will allow to regularly save checkpoint (save/reload teh model, ...)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "     \n",
    "    \n",
    "    #####  Before Starting, Always initialize the variables defined in the computation graph\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Monitor execution time\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    \n",
    "    ##### Train the model\n",
    "    for iteration in range(epoch):\n",
    "        \n",
    "        #### Get a batch of images ow which to train\n",
    "    \n",
    "        # We use the Tensorflow MNIST tutorial helpers to get a \n",
    "        # batch of images on which to perform the training iteration\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Define a variable for convenience to store the Feed dictionary for training\n",
    "        train_feed_dict = { inputs: batch_xs, targets: batch_ys }    \n",
    "\n",
    "        \n",
    "        #### This line actually performs the Training (by evaluating train_op, which is the optimizer in the graph)\n",
    "        sess.run(train_op, feed_dict= train_feed_dict)\n",
    "        \n",
    "        \n",
    "        #### Every 100 iterations, collect the Summary logs for Tensorboard and \n",
    "        #### evaluate the Accuracy of the current model against a TEST dataset (instead of the Training dataset)\n",
    "        #### and print progress info  \n",
    "    \n",
    "        if iteration % 100 == 0:\n",
    "            [train_accuracy, summary] = sess.run([accuracy, summary_op], feed_dict = train_feed_dict)\n",
    "            train_writer.add_summary(summary, iteration)\n",
    "                 \n",
    "            # Now evaluate the Test Accuracy using the test dataset Test\n",
    "            [test_accuracy, summary] = sess.run([accuracy, summary_op], feed_dict = {inputs: mnist.test.images, targets: mnist.test.labels})\n",
    "            test_writer.add_summary(summary, iteration)\n",
    "            \n",
    "            print(\"iteration: \", iteration, \"  training accuracy=\", train_accuracy, \"  test accuracy=\", test_accuracy)\n",
    "            \n",
    "    \n",
    "    \n",
    "      #### Every 1000 iterations write a checkpoint for our model\n",
    "        if iteration % 1000 == 0:\n",
    "            saver.save(sess, os.path.join(str(logs_path +\"-train-\" + str(experiments)), \"model.ckpt\"), iteration)\n",
    "        \n",
    "\n",
    "   \n",
    "    #### Training is done !\n",
    "    print(\"Execution time= %4f sec\" % (timeit.default_timer() - start_time)) \n",
    "          \n",
    "    \n",
    "    # Not important : increment our counter to avoid mixing up with our logs between experiments in Jupyter\n",
    "    experiments +=1\n",
    "\n",
    "    #### Gently close the opened writers and sesion\n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    sess.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualize the training summaries in Tensorboard\n",
    "\n",
    "- in a terminal launch\n",
    "```\n",
    "tensorboard --logdir ./logs\n",
    "``` \n",
    "- open tensorboard in a browser : http://localhost:6006/\n",
    "\n",
    "Now have a look at :\n",
    "    - the image section : you can use this section to visualize some images used in this experiment\n",
    "    - the graphs : you can visualize the Tensorflow graph (usefull to look for errors)\n",
    "    - the Scalar section : you can visualize  here the accuracy and loss (cross-entropy) and see how well or not your model is performing : \n",
    "        - Does it learn ? Does it learn fast ?\n",
    "        - Does it Underfit ? Does it Overfit ?\n",
    "        - Is it an accurate model ?\n",
    "        - How do several models/hyper-parameters variations compare to each others ?\n",
    "        - And much more !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
