{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Techforum : Deep Learning (part 3/3)\n",
    "\n",
    "## Convolutional Neural Networks (Convnets) in Keras\n",
    "\n",
    "Objective:\n",
    "- Now using Deep Learning to get better results at classifying MNIST\n",
    "\n",
    "Note : this toy-example run faster on a GPU... and much much slower on CPU (~45 minutes on a laptop !)\n",
    "\n",
    "Notebook inspired by : https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import timeit\n",
    "\n",
    "# Use Tensorflow tutorial's helper to load/prepare the MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Import keras and even more Deep Learning / Convolutional Neural network buildling blocks\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import data (Thanks to helpers provided in Tensorflow tutorials !)\n",
    "mnist = input_data.read_data_sets('./', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define for convenience a few (Python/Numpy)variables to handle the dataset \n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "# Reshape the dataset to fit with the convolutional layers\n",
    "# which expect at their input a 4D tensor with shape\n",
    "# (samples, rows, cols, channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some Hyperparameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How fast the network will learn, by making more or less small updates during training\n",
    "#    too low, and the network will take too much time to learn\n",
    "#    too high, and the network might never converge to a solution\n",
    "learning_rate = 0.001 \n",
    "\n",
    "# Number of training epoch (in Keras : ~loop on the full training dataset)\n",
    "epoch = 12\n",
    "\n",
    "# Number of iamges to process per batch iteration\n",
    "batch_size = 128\n",
    "\n",
    "# Path to home of the Tensorboard logs and Training Checkpoints\n",
    "logs_path = \"./logs/mnist/Keras/ConvNet\" \n",
    "\n",
    "# Not important : Just a counter to separate logs directory between each training experiments\n",
    "experiments = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build the model : just by stacking the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Sequential model is a linear stack of layers\n",
    "model = Sequential()\n",
    "\n",
    "# 2D convolution layer (spatial convolution over images).\n",
    "# This layer creates a convolution kernel that is convolved with the \n",
    "# layer input to produce a tensor of outputs.\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)))\n",
    "\n",
    "# Apply a second layer of convolution\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), \n",
    "                 activation='relu'))\n",
    "\n",
    "# Sub-sampling : max pooling operation for spatial data.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add regularization : Dropout consists in randomly setting a fraction rate \n",
    "# of input units to 0 at each update during training time which helps prevent overfitting\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flattens the input. Does not affect the batch size.\n",
    "model.add(Flatten())\n",
    "\n",
    "# Just a regular densely-connected NN layer, here with 128 neurons\n",
    "# relu (rectified linear unit) is applying a non-linearity \n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "# Add regularization : Dropout consists in randomly setting a fraction rate \n",
    "# of input units to 0 at each update during training time which helps prevent overfitting\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "# Just a regular densely-connected NN layer for the ouput of the network, here with 10 neurons\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cool tool to display information about the model we have built\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training in Keras : compile, fit (train), evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "55000/55000 [==============================] - 10s - loss: 0.3498 - acc: 0.8926 - val_loss: 0.0806 - val_acc: 0.9739\n",
      "Epoch 2/12\n",
      "55000/55000 [==============================] - 9s - loss: 0.1191 - acc: 0.9651 - val_loss: 0.0546 - val_acc: 0.9823\n",
      "Epoch 3/12\n",
      "55000/55000 [==============================] - 9s - loss: 0.0916 - acc: 0.9728 - val_loss: 0.0443 - val_acc: 0.9852\n",
      "Epoch 4/12\n",
      "55000/55000 [==============================] - 9s - loss: 0.0751 - acc: 0.9774 - val_loss: 0.0384 - val_acc: 0.9873\n",
      "Epoch 5/12\n",
      "55000/55000 [==============================] - 9s - loss: 0.0649 - acc: 0.9808 - val_loss: 0.0366 - val_acc: 0.9877\n",
      "Epoch 6/12\n",
      "55000/55000 [==============================] - 9s - loss: 0.0566 - acc: 0.9827 - val_loss: 0.0351 - val_acc: 0.9880\n",
      "Epoch 7/12\n",
      "55000/55000 [==============================] - 9s - loss: 0.0528 - acc: 0.9846 - val_loss: 0.0304 - val_acc: 0.9889\n",
      "Epoch 8/12\n",
      "55000/55000 [==============================] - 9s - loss: 0.0473 - acc: 0.9855 - val_loss: 0.0314 - val_acc: 0.9896\n",
      "Epoch 9/12\n",
      "55000/55000 [==============================] - 9s - loss: 0.0435 - acc: 0.9873 - val_loss: 0.0302 - val_acc: 0.9887\n",
      "Epoch 10/12\n",
      "55000/55000 [==============================] - 9s - loss: 0.0422 - acc: 0.9875 - val_loss: 0.0293 - val_acc: 0.9903\n",
      "Epoch 11/12\n",
      "55000/55000 [==============================] - 9s - loss: 0.0399 - acc: 0.9880 - val_loss: 0.0289 - val_acc: 0.9893\n",
      "Epoch 12/12\n",
      "55000/55000 [==============================] - 9s - loss: 0.0380 - acc: 0.9884 - val_loss: 0.0285 - val_acc: 0.9906\n",
      "Test loss: 0.0285407811684\n",
      "Test accuracy: 0.9906\n",
      "Execution time= 118.637069 sec\n"
     ]
    }
   ],
   "source": [
    "tbCallBack = keras.callbacks.TensorBoard(log_dir=logs_path, histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Monitor execution time\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epoch,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[tbCallBack])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#### Training is done !\n",
    "\n",
    "print(\"Execution time= %4f sec\" % (timeit.default_timer() - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
